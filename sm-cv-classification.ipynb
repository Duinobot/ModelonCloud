{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50f9bb39-23bc-4b94-b318-a7484368a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cats and Dogs image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "824ac0da-ad57-4646-922c-a514ccc018bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get session and buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c3618c0-a4f9-4745-a211-94acaca9b22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.107.0\n",
      "sagemaker-us-east-1-829135631045\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b82cb02d-c63d-4925-a1cf-50da4b936178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Using image format\n",
    "# prefix = 'dogscats-images'\n",
    "\n",
    "# s3_train_path = 's3://{}/{}/input/train/'.format(bucket, prefix)\n",
    "# s3_val_path = 's3://{}/{}/input/val/'.format(bucket, prefix)\n",
    "# s3_train_lst_path = 's3://{}/{}/input/train_lst/'.format(bucket, prefix)\n",
    "# s3_val_lst_path = 's3://{}/{}/input/val_lst/'.format(bucket, prefix)\n",
    "\n",
    "# s3_output = 's3://{}/{}/output/'.format(bucket, prefix)\n",
    "\n",
    "# print(s3_train_path)\n",
    "# print(s3_val_path)\n",
    "# print(s3_train_lst_path)\n",
    "# print(s3_val_lst_path)\n",
    "# print(s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9bffdbe6-061f-4c2e-8d97-edbcd9e8666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-829135631045/dogscats/input/train/\n",
      "s3://sagemaker-us-east-1-829135631045/dogscats/input/validation/\n"
     ]
    }
   ],
   "source": [
    "# 2 Train using recordIO\n",
    "prefix = 'dogscats'\n",
    "s3_train_path = 's3://{}/{}/input/train/'.format(bucket, prefix)\n",
    "s3_val_path = 's3://{}/{}/input/validation/'.format(bucket, prefix)\n",
    "\n",
    "s3_output='s3://{}/{}/output/'.format(bucket, prefix)\n",
    "print(s3_train_path)\n",
    "print(s3_val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d3abddb-08a3-45fe-bab8-5b2332093050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve image and configure training estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f70128b6-e886-4323-9161-67416beed656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811284229777.dkr.ecr.us-east-1.amazonaws.com/image-classification:1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "region = sess.boto_session.region_name    \n",
    "container = retrieve('image-classification', region)\n",
    "print(container)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "268e1709-acb7-4fa9-aa77-af2cba993026",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    sagemaker.get_execution_role(), \n",
    "    instance_count=1, \n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    output_path=s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fa473e10-8076-4954-aed4-5f84e73a4777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.estimator.Estimator at 0x7fe44eb7cd10>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e49e4ebd-65c5-4014-832a-f20b43d16ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparam for training job\n",
    "ic.set_hyperparameters(\n",
    "    num_layers=18,               # Train a Resnet-18 model\n",
    "    use_pretrained_model=0,      # Train from scratch\n",
    "    num_classes=2,               # Dogs and cats\n",
    "    num_training_samples=22500,  # Number of training samples\n",
    "    mini_batch_size=128,\n",
    "    resize=224,\n",
    "    epochs=10)                   # Learn the training set 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3a390aa9-e674-42fc-8a2c-4743090c8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1 Using image format\n",
    "# from sagemaker import TrainingInput\n",
    "\n",
    "# train_data = TrainingInput(\n",
    "#     s3_train_path, \n",
    "#     content_type='application/x-image')\n",
    "\n",
    "# val_data = TrainingInput(\n",
    "#     s3_val_path, \n",
    "#     content_type='application/x-image')\n",
    "\n",
    "# train_lst_data = TrainingInput(\n",
    "#     s3_train_lst_path, \n",
    "#     content_type='application/x-image')\n",
    "\n",
    "# val_lst_data = TrainingInput(\n",
    "#     s3_val_lst_path, \n",
    "#     content_type='application/x-image')\n",
    "\n",
    "# s3_channels = {'train': train_data, 'validation': val_data, \n",
    "#                'train_lst': train_lst_data, 'validation_lst': val_lst_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "31171387-3fe6-431a-9158-a8eaedd53072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Using RecordIO\n",
    "from sagemaker import TrainingInput\n",
    "\n",
    "train_data = TrainingInput(\n",
    "    s3_train_path,\n",
    "    content_type='application/x-recordio')\n",
    "validation_data = TrainingInput(\n",
    "    s3_val_path,                                        \n",
    "    content_type='application/x-recordio')\n",
    "\n",
    "s3_channels = {'train': train_data,\n",
    "               'validation': validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e4771d4a-a145-45b4-a2b1-1327afd8ff7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-11 00:38:53 Starting - Starting the training job...ProfilerReport-1662856732: InProgress\n",
      "...\n",
      "2022-09-11 00:39:36 Starting - Preparing the instances for training......\n",
      "2022-09-11 00:40:37 Downloading - Downloading input data...\n",
      "2022-09-11 00:41:17 Training - Downloading the training image.............\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/image_classification/default-input.json: {'use_pretrained_model': 0, 'num_layers': 152, 'epochs': 30, 'learning_rate': 0.1, 'lr_scheduler_factor': 0.1, 'optimizer': 'sgd', 'momentum': 0, 'weight_decay': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'gamma': 0.9, 'mini_batch_size': 32, 'image_shape': '3,224,224', 'precision_dtype': 'float32'}\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'epochs': '10', 'mini_batch_size': '128', 'num_classes': '2', 'num_layers': '18', 'num_training_samples': '22500', 'resize': '224', 'use_pretrained_model': '0'}\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] Final configuration: {'use_pretrained_model': '0', 'num_layers': '18', 'epochs': '10', 'learning_rate': 0.1, 'lr_scheduler_factor': 0.1, 'optimizer': 'sgd', 'momentum': 0, 'weight_decay': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'eps': 1e-08, 'gamma': 0.9, 'mini_batch_size': '128', 'image_shape': '3,224,224', 'precision_dtype': 'float32', 'num_classes': '2', 'num_training_samples': '22500', 'resize': '224'}\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] Searching for .rec files in /opt/ml/input/data/train.\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] Searching for .rec files in /opt/ml/input/data/validation.\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] use_pretrained_model: 0\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] multi_label: 0\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] Performing random weight initialization\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] ---- Parameters ----\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] num_layers: 18\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] data type: <class 'numpy.float32'>\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] epochs: 10\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] image resize size: 224\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] optimizer: sgd\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] momentum: 0.9\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] weight_decay: 0.0001\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] learning_rate: 0.1\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] num_training_samples: 22500\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] mini_batch_size: 128\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] image_shape: 3,224,224\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] num_classes: 2\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] augmentation_type: None\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] kv_store: device\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] checkpoint_frequency not set, will store the best model\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] --------------------\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:32 INFO 140408845727552] Setting number of threads: 7\u001b[0m\n",
      "\n",
      "2022-09-11 00:43:37 Training - Training image download completed. Training in progress.\u001b[34m[00:43:44] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.4.x_ecl_Cuda_11.1.x.19.0/AL2_x86_64/generic-flavor/src/src/operator/nn/./cudnn/./cudnn_algoreg-inl.h:97: Running performance tests to find the best convolution algorithm, this can take a while... (setting env variable MXNET_CUDNN_AUTOTUNE_DEFAULT to 0 to disable)\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:49 INFO 140408845727552] Epoch[0] Batch [20]#011Speed: 542.170 samples/sec#011accuracy=0.533482\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:51 INFO 140408845727552] Epoch[0] Batch [40]#011Speed: 690.252 samples/sec#011accuracy=0.553925\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:54 INFO 140408845727552] Epoch[0] Batch [60]#011Speed: 759.837 samples/sec#011accuracy=0.569416\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:57 INFO 140408845727552] Epoch[0] Batch [80]#011Speed: 799.773 samples/sec#011accuracy=0.590181\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:43:59 INFO 140408845727552] Epoch[0] Batch [100]#011Speed: 825.944 samples/sec#011accuracy=0.604657\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:02 INFO 140408845727552] Epoch[0] Batch [120]#011Speed: 843.569 samples/sec#011accuracy=0.613701\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:05 INFO 140408845727552] Epoch[0] Batch [140]#011Speed: 856.035 samples/sec#011accuracy=0.620623\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:08 INFO 140408845727552] Epoch[0] Batch [160]#011Speed: 866.693 samples/sec#011accuracy=0.629319\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:09 INFO 140408845727552] Epoch[0] Train-accuracy=0.634554\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:09 INFO 140408845727552] Epoch[0] Time cost=25.455\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:12 INFO 140408845727552] Epoch[0] Validation-accuracy=0.709375\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:12 INFO 140408845727552] Storing the best model with validation accuracy: 0.709375\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:12 INFO 140408845727552] Saved checkpoint to \"/opt/ml/model/image-classification-0001.params\"\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:15 INFO 140408845727552] Epoch[1] Batch [20]#011Speed: 862.058 samples/sec#011accuracy=0.717262\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:18 INFO 140408845727552] Epoch[1] Batch [40]#011Speed: 903.371 samples/sec#011accuracy=0.712081\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:21 INFO 140408845727552] Epoch[1] Batch [60]#011Speed: 919.224 samples/sec#011accuracy=0.713371\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:23 INFO 140408845727552] Epoch[1] Batch [80]#011Speed: 926.270 samples/sec#011accuracy=0.718268\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:26 INFO 140408845727552] Epoch[1] Batch [100]#011Speed: 930.829 samples/sec#011accuracy=0.723700\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:29 INFO 140408845727552] Epoch[1] Batch [120]#011Speed: 933.573 samples/sec#011accuracy=0.727983\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:31 INFO 140408845727552] Epoch[1] Batch [140]#011Speed: 935.434 samples/sec#011accuracy=0.729998\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:34 INFO 140408845727552] Epoch[1] Batch [160]#011Speed: 936.938 samples/sec#011accuracy=0.734472\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:36 INFO 140408845727552] Epoch[1] Train-accuracy=0.736429\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:36 INFO 140408845727552] Epoch[1] Time cost=23.733\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:38 INFO 140408845727552] Epoch[1] Validation-accuracy=0.727734\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:38 INFO 140408845727552] Storing the best model with validation accuracy: 0.727734\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:38 INFO 140408845727552] Saved checkpoint to \"/opt/ml/model/image-classification-0002.params\"\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:41 INFO 140408845727552] Epoch[2] Batch [20]#011Speed: 833.553 samples/sec#011accuracy=0.765253\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:44 INFO 140408845727552] Epoch[2] Batch [40]#011Speed: 858.340 samples/sec#011accuracy=0.762576\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:47 INFO 140408845727552] Epoch[2] Batch [60]#011Speed: 877.868 samples/sec#011accuracy=0.767290\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:50 INFO 140408845727552] Epoch[2] Batch [80]#011Speed: 889.939 samples/sec#011accuracy=0.771798\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:53 INFO 140408845727552] Epoch[2] Batch [100]#011Speed: 899.986 samples/sec#011accuracy=0.775758\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:55 INFO 140408845727552] Epoch[2] Batch [120]#011Speed: 908.416 samples/sec#011accuracy=0.780088\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:44:58 INFO 140408845727552] Epoch[2] Batch [140]#011Speed: 914.243 samples/sec#011accuracy=0.782358\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:01 INFO 140408845727552] Epoch[2] Batch [160]#011Speed: 918.480 samples/sec#011accuracy=0.784792\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:03 INFO 140408845727552] Epoch[2] Train-accuracy=0.786920\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:03 INFO 140408845727552] Epoch[2] Time cost=24.212\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:05 INFO 140408845727552] Epoch[2] Validation-accuracy=0.775082\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:05 INFO 140408845727552] Storing the best model with validation accuracy: 0.775082\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:05 INFO 140408845727552] Saved checkpoint to \"/opt/ml/model/image-classification-0003.params\"\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:08 INFO 140408845727552] Epoch[3] Batch [20]#011Speed: 868.625 samples/sec#011accuracy=0.822917\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:11 INFO 140408845727552] Epoch[3] Batch [40]#011Speed: 903.535 samples/sec#011accuracy=0.819169\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:14 INFO 140408845727552] Epoch[3] Batch [60]#011Speed: 917.959 samples/sec#011accuracy=0.819800\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:16 INFO 140408845727552] Epoch[3] Batch [80]#011Speed: 926.071 samples/sec#011accuracy=0.820698\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:19 INFO 140408845727552] Epoch[3] Batch [100]#011Speed: 927.454 samples/sec#011accuracy=0.822169\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:22 INFO 140408845727552] Epoch[3] Batch [120]#011Speed: 930.134 samples/sec#011accuracy=0.824897\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:25 INFO 140408845727552] Epoch[3] Batch [140]#011Speed: 933.421 samples/sec#011accuracy=0.827626\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:27 INFO 140408845727552] Epoch[3] Batch [160]#011Speed: 935.355 samples/sec#011accuracy=0.830115\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:29 INFO 140408845727552] Epoch[3] Train-accuracy=0.832366\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:29 INFO 140408845727552] Epoch[3] Time cost=23.783\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:32 INFO 140408845727552] Epoch[3] Validation-accuracy=0.766406\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:35 INFO 140408845727552] Epoch[4] Batch [20]#011Speed: 879.066 samples/sec#011accuracy=0.857143\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:37 INFO 140408845727552] Epoch[4] Batch [40]#011Speed: 903.646 samples/sec#011accuracy=0.854040\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:40 INFO 140408845727552] Epoch[4] Batch [60]#011Speed: 902.416 samples/sec#011accuracy=0.853740\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:43 INFO 140408845727552] Epoch[4] Batch [80]#011Speed: 909.320 samples/sec#011accuracy=0.855806\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:46 INFO 140408845727552] Epoch[4] Batch [100]#011Speed: 916.756 samples/sec#011accuracy=0.856668\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:48 INFO 140408845727552] Epoch[4] Batch [120]#011Speed: 922.528 samples/sec#011accuracy=0.857438\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:51 INFO 140408845727552] Epoch[4] Batch [140]#011Speed: 926.049 samples/sec#011accuracy=0.860206\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:54 INFO 140408845727552] Epoch[4] Batch [160]#011Speed: 928.926 samples/sec#011accuracy=0.861656\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:56 INFO 140408845727552] Epoch[4] Train-accuracy=0.863571\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:56 INFO 140408845727552] Epoch[4] Time cost=23.929\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:58 INFO 140408845727552] Epoch[4] Validation-accuracy=0.823602\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:58 INFO 140408845727552] Storing the best model with validation accuracy: 0.823602\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:45:58 INFO 140408845727552] Saved checkpoint to \"/opt/ml/model/image-classification-0005.params\"\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:01 INFO 140408845727552] Epoch[5] Batch [20]#011Speed: 891.870 samples/sec#011accuracy=0.883557\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:04 INFO 140408845727552] Epoch[5] Batch [40]#011Speed: 912.635 samples/sec#011accuracy=0.882241\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:07 INFO 140408845727552] Epoch[5] Batch [60]#011Speed: 914.187 samples/sec#011accuracy=0.878586\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:09 INFO 140408845727552] Epoch[5] Batch [80]#011Speed: 923.016 samples/sec#011accuracy=0.882716\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:12 INFO 140408845727552] Epoch[5] Batch [100]#011Speed: 927.770 samples/sec#011accuracy=0.883973\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:15 INFO 140408845727552] Epoch[5] Batch [120]#011Speed: 931.087 samples/sec#011accuracy=0.883071\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:17 INFO 140408845727552] Epoch[5] Batch [140]#011Speed: 932.752 samples/sec#011accuracy=0.885361\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:20 INFO 140408845727552] Epoch[5] Batch [160]#011Speed: 934.524 samples/sec#011accuracy=0.888441\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:22 INFO 140408845727552] Epoch[5] Train-accuracy=0.888839\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:22 INFO 140408845727552] Epoch[5] Time cost=23.789\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:25 INFO 140408845727552] Epoch[5] Validation-accuracy=0.830859\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:25 INFO 140408845727552] Storing the best model with validation accuracy: 0.830859\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:25 INFO 140408845727552] Saved checkpoint to \"/opt/ml/model/image-classification-0006.params\"\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:28 INFO 140408845727552] Epoch[6] Batch [20]#011Speed: 871.070 samples/sec#011accuracy=0.909226\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:30 INFO 140408845727552] Epoch[6] Batch [40]#011Speed: 909.522 samples/sec#011accuracy=0.902439\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:33 INFO 140408845727552] Epoch[6] Batch [60]#011Speed: 923.526 samples/sec#011accuracy=0.902152\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:36 INFO 140408845727552] Epoch[6] Batch [80]#011Speed: 929.582 samples/sec#011accuracy=0.908275\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:38 INFO 140408845727552] Epoch[6] Batch [100]#011Speed: 933.657 samples/sec#011accuracy=0.910272\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:41 INFO 140408845727552] Epoch[6] Batch [120]#011Speed: 936.756 samples/sec#011accuracy=0.911609\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:44 INFO 140408845727552] Epoch[6] Batch [140]#011Speed: 938.785 samples/sec#011accuracy=0.914450\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:47 INFO 140408845727552] Epoch[6] Batch [160]#011Speed: 940.600 samples/sec#011accuracy=0.915906\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:48 INFO 140408845727552] Epoch[6] Train-accuracy=0.916250\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:48 INFO 140408845727552] Epoch[6] Time cost=23.662\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:51 INFO 140408845727552] Epoch[6] Validation-accuracy=0.863076\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:51 INFO 140408845727552] Storing the best model with validation accuracy: 0.863076\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:51 INFO 140408845727552] Saved checkpoint to \"/opt/ml/model/image-classification-0007.params\"\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:54 INFO 140408845727552] Epoch[7] Batch [20]#011Speed: 880.119 samples/sec#011accuracy=0.931548\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:57 INFO 140408845727552] Epoch[7] Batch [40]#011Speed: 912.467 samples/sec#011accuracy=0.925114\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:46:59 INFO 140408845727552] Epoch[7] Batch [60]#011Speed: 923.479 samples/sec#011accuracy=0.926614\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:02 INFO 140408845727552] Epoch[7] Batch [80]#011Speed: 928.402 samples/sec#011accuracy=0.929495\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:05 INFO 140408845727552] Epoch[7] Batch [100]#011Speed: 928.099 samples/sec#011accuracy=0.929688\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:08 INFO 140408845727552] Epoch[7] Batch [120]#011Speed: 931.872 samples/sec#011accuracy=0.929946\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:10 INFO 140408845727552] Epoch[7] Batch [140]#011Speed: 934.422 samples/sec#011accuracy=0.931904\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:13 INFO 140408845727552] Epoch[7] Batch [160]#011Speed: 936.838 samples/sec#011accuracy=0.933036\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:15 INFO 140408845727552] Epoch[7] Train-accuracy=0.933973\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:15 INFO 140408845727552] Epoch[7] Time cost=23.744\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:17 INFO 140408845727552] Epoch[7] Validation-accuracy=0.863672\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:17 INFO 140408845727552] Storing the best model with validation accuracy: 0.863672\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:17 INFO 140408845727552] Saved checkpoint to \"/opt/ml/model/image-classification-0008.params\"\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:20 INFO 140408845727552] Epoch[8] Batch [20]#011Speed: 854.866 samples/sec#011accuracy=0.939732\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:23 INFO 140408845727552] Epoch[8] Batch [40]#011Speed: 891.510 samples/sec#011accuracy=0.938072\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:26 INFO 140408845727552] Epoch[8] Batch [60]#011Speed: 909.123 samples/sec#011accuracy=0.939549\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:29 INFO 140408845727552] Epoch[8] Batch [80]#011Speed: 919.781 samples/sec#011accuracy=0.944927\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:31 INFO 140408845727552] Epoch[8] Batch [100]#011Speed: 925.804 samples/sec#011accuracy=0.945158\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:34 INFO 140408845727552] Epoch[8] Batch [120]#011Speed: 930.112 samples/sec#011accuracy=0.943505\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:37 INFO 140408845727552] Epoch[8] Batch [140]#011Speed: 932.478 samples/sec#011accuracy=0.941766\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:39 INFO 140408845727552] Epoch[8] Batch [160]#011Speed: 933.909 samples/sec#011accuracy=0.942595\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:41 INFO 140408845727552] Epoch[8] Train-accuracy=0.943438\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:41 INFO 140408845727552] Epoch[8] Time cost=23.896\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:44 INFO 140408845727552] Epoch[8] Validation-accuracy=0.845806\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:47 INFO 140408845727552] Epoch[9] Batch [20]#011Speed: 835.955 samples/sec#011accuracy=0.952381\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:50 INFO 140408845727552] Epoch[9] Batch [40]#011Speed: 887.595 samples/sec#011accuracy=0.951410\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:52 INFO 140408845727552] Epoch[9] Batch [60]#011Speed: 908.990 samples/sec#011accuracy=0.949667\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:55 INFO 140408845727552] Epoch[9] Batch [80]#011Speed: 917.754 samples/sec#011accuracy=0.951196\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:47:58 INFO 140408845727552] Epoch[9] Batch [100]#011Speed: 923.327 samples/sec#011accuracy=0.950031\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:48:00 INFO 140408845727552] Epoch[9] Batch [120]#011Speed: 927.543 samples/sec#011accuracy=0.949961\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:48:03 INFO 140408845727552] Epoch[9] Batch [140]#011Speed: 928.806 samples/sec#011accuracy=0.951518\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:48:06 INFO 140408845727552] Epoch[9] Batch [160]#011Speed: 929.650 samples/sec#011accuracy=0.952446\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:48:08 INFO 140408845727552] Epoch[9] Train-accuracy=0.953393\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:48:08 INFO 140408845727552] Epoch[9] Time cost=23.909\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:48:10 INFO 140408845727552] Epoch[9] Validation-accuracy=0.914453\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:48:10 INFO 140408845727552] Storing the best model with validation accuracy: 0.914453\u001b[0m\n",
      "\u001b[34m[09/11/2022 00:48:10 INFO 140408845727552] Saved checkpoint to \"/opt/ml/model/image-classification-0010.params\"\u001b[0m\n",
      "\n",
      "2022-09-11 00:48:18 Uploading - Uploading generated training model\n",
      "2022-09-11 00:48:38 Completed - Training job completed\n",
      "ProfilerReport-1662856732: NoIssuesFound\n",
      "Training seconds: 483\n",
      "Billable seconds: 483\n"
     ]
    }
   ],
   "source": [
    "ic.fit(inputs=s3_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79469c4d-1833-430c-ada4-440a0b0f66fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------!"
     ]
    }
   ],
   "source": [
    "ic_predictor = ic.deploy(initial_instance_count=1,\n",
    "                         instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8f32c83-8ab9-4067-819e-c8e14a7fd1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7849ef55-fb49-421b-ba4b-69c99507545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.jpg', 'rb') as f:\n",
    "    payload = f.read()\n",
    "    payload = bytearray(payload)\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=ic_predictor.endpoint_name,\n",
    "    Body=payload,\n",
    "    ContentType='application/x-image',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d739fa49-74be-42d2-933a-7d16f59970f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'[0.006279913242906332, 0.9937200546264648]'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = response['Body'].read()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e51d4f78-c829-428b-96c0-b1d30061cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c788db06-d34f-4743-a364-98942c733291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.006279913242906332, 0.9937200546264648]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "750a7bb7-1d62-4de1-a6a6-05798425dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argmax(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d34e626-3b69-4b33-815f-9a79c6390c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9937200546264648 1\n"
     ]
    }
   ],
   "source": [
    "print(result[index], index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b660e509-cbfe-4d8d-b4fd-a516cc7902b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed8b5b-70db-4d5e-982a-c69e6176ea91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
